{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as sf\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql.functions import col, avg, date_format,month,hour,lag, date_sub,lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType, DoubleType\n",
    "from pyspark.sql.functions import broadcast\n",
    "import pandas as pd\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, OneHotEncoder\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://172.16.27.208:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://172.16.27.208:7077 appName=spark>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop()\n",
    "sc = pyspark.SparkContext(master=\"spark://172.16.27.208:7077\",appName=\"spark\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(resampled_df,window_period,week_lag):\n",
    "    # window period = 6 for lag input for same hour,same-1,same-2 \n",
    "    window = Window.partitionBy('cluster_id').orderBy('date','hour')\n",
    "    for lag_hour in range(0,window_period+1):\n",
    "        for diff in range(1,3):\n",
    "            resampled_df = resampled_df.withColumn('{}_diff_energy_t_{}'.format(diff,lag_hour),lag(resampled_df['energy(kWh/h)'], count=24*diff+lag_hour).over(window)) \n",
    "    for lag_week in range(1,week_lag+1):\n",
    "        resampled_df = resampled_df.withColumn('diff_energy_week_t_{}'.format(lag_week),lag(resampled_df['energy(kWh/h)'], count=24*7*lag_week).over(window)) \n",
    "    df_resample_lag = resampled_df\n",
    "\n",
    "    # Mean of previous 6 days\n",
    "    df_resample_lag = df_resample_lag.withColumn(\"rnk\",sf.dense_rank().over(Window.partitionBy('cluster_id').orderBy('date')))\n",
    "    for days in range(1,window_period+1):\n",
    "        df_resample_lag = df_resample_lag.withColumn(\"mean_{}\".format(days),avg(\"energy(kWh/h)\").over(Window.partitionBy(\"cluster_id\").orderBy(\"rnk\").rangeBetween(-days,-days)))\n",
    "#     df_resample_lag.show()\n",
    "    # Min power of previous 2 days\n",
    "    for days in range(1,window_period+1):\n",
    "        df_resample_lag = df_resample_lag.withColumn(\"min_{}\".format(days),sf.min(\"energy(kWh/h)\").over(Window.partitionBy(\"cluster_id\").orderBy(\"rnk\").rangeBetween(-days,-days)))\n",
    "        df_resample_lag = df_resample_lag.withColumn(\"max_{}\".format(days),sf.max(\"energy(kWh/h)\").over(Window.partitionBy(\"cluster_id\").orderBy(\"rnk\").rangeBetween(-days,-days)))\n",
    "    \n",
    "    return df_resample_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weather_feature(df):\n",
    "    weather_data = sqlcontext.read.csv(base_path+\"weather_hourly_darksky.csv\",header=True,inferSchema=True)\n",
    "    weather_daily_data = sqlcontext.read.csv(base_path+\"weather_daily_darksky.csv\",header=True,inferSchema=True)\n",
    "    weather_daily_data = weather_daily_data.select(date_format(\"temperatureMaxTime\",\"yyyy-MM-dd\").alias(\"date2\"),\"temperatureMax\",\"temperatureMin\")\n",
    "    weather_data = weather_data.withColumn(\"hour1\",hour(weather_data[\"time\"]))\n",
    "    weather_data = weather_data.withColumn(\"date1\",date_format(weather_data[\"time\"],\"yyyy-MM-dd\").cast(DateType()))\n",
    "    weather_data = weather_data.drop(\"time\",\"icon\",\"temperature\")\n",
    "    weather_data = weather_data.join(broadcast(weather_daily_data),(weather_daily_data.date2 == weather_data.date1))\n",
    "    weather_data.printSchema()\n",
    "    df_full_dataset = df.join(broadcast(weather_data),(df.date == weather_data.date1) & (df.hour == weather_data.hour1))\n",
    "    df_full_dataset = df_full_dataset.drop(\"hour1\",\"date1\").cache()\n",
    "    df_full_dataset.take(1)\n",
    "    df_full_dataset = df_full_dataset.na.drop()\n",
    "    return df_full_dataset\n",
    "\n",
    "def add_holiday_feature(df):\n",
    "    holiday_data = sqlcontext.read.csv(base_path+\"uk_bank_holidays.csv\",header=True,inferSchema=True)\n",
    "    holiday_data = holiday_data.withColumn(\"Bank holidays\",date_format(holiday_data[\"Bank holidays\"],\"yyyy-MM-dd\").cast(DateType()))\n",
    "    holiday_data = holiday_data.select(\"Bank holidays\")\n",
    "    holiday_data = holiday_data.withColumn(\"holiday\",lit(1))\n",
    "    feature_df = df.join(holiday_data,holiday_data[\"Bank holidays\"] == df[\"date\"],how=\"left\")\n",
    "    feature_df = feature_df.fillna({'holiday':'0'})\n",
    "    feature_df = feature_df.drop(\"Bank holidays\")\n",
    "    feature_df = feature_df.withColumn(\"Weekday/end\",sf.when((col(\"weekDay\")==str(\"Sat\")) | (col(\"weekDay\")==str(\"Sat\")),1).otherwise(0))\n",
    "    feature_df = feature_df.na.drop()\n",
    "    return feature_df\n",
    "\n",
    "# def acorn_info(df,household_info):\n",
    "#     Acorn_data_group = household_info.select(\"LCLid\",\"stdorToU\",\"Acorn_grouped\")\n",
    "#     Acorn_data_group.select(\"Acorn_grouped\").distinct().collect()\n",
    "#     possible_group = [\"Comfortable\",\"Affluent\",\"Adversity\"]\n",
    "#     Acorn_data_group = Acorn_data_group.filter(Acorn_data_group.Acorn_grouped.isin(possible_group))\n",
    "#     feature_df = df.join(Acorn_data_group,[\"LCLid\"])                # preventing duplicate column in df\n",
    "#     feature_df.printSchema()\n",
    "#     #Acorn_data_group.select(\"stdorToU\").distinct().collect()\n",
    "#     feature_df.take(1)\n",
    "#     return feature_df\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/test5/Desktop/smart-meters-in-london/\"\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_info = sqlcontext.read.csv(base_path+\"informations_households.csv\",header=True,inferSchema=True)\n",
    "household_mini = household_info\n",
    "# household_mini = household_info.limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- stdorToU: string (nullable = true)\n",
      " |-- Acorn: string (nullable = true)\n",
      " |-- Acorn_grouped: string (nullable = true)\n",
      " |-- file: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "household_mini.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_1_hr(df):\n",
    "    df = df.select(\"LCLid\",\"tstp\",\"energy(kWh/hh)\",date_format(\"tstp\",\"yyyy-MM-dd\").alias(\"date\"),date_format(\"tstp\",'HH:mm').alias(\"start time\"),date_format(\"tstp\",'E').alias(\"weekDay\"),month(\"tstp\").alias(\"month\"),hour(\"tstp\").alias(\"hour\"))\n",
    "    df = df.withColumn(\"energy(kWh/hh)\",df[\"energy(kWh/hh)\"].cast(\"float\"))\n",
    "    df1 = (df.groupby('LCLid',\"date\",\"hour\").sum(\"energy(kWh/hh)\")).orderBy('date','hour',ascending=True)\n",
    "    df1 = df1.withColumnRenamed(\"sum(energy(kWh/hh))\",\"energy(kWh/h)\")\n",
    "    resampled_df = df1.select(\"LCLid\",\"date\",\"hour\",\"energy(kWh/h)\",month(\"date\").alias(\"month\"),date_format(\"date\",'E').alias(\"weekDay\"))\n",
    "    resampled_df = resampled_df.withColumn(\"energy(kWh/h)\", sf.round(resampled_df[\"energy(kWh/h)\"], 3))\n",
    "    resampled_df = resampled_df.withColumn(\"date\", resampled_df[\"date\"].cast(DateType()))\n",
    "    return resampled_df\n",
    "# resampled_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|    file|\n",
      "+--------+\n",
      "|block_64|\n",
      "|block_91|\n",
      "|block_39|\n",
      "|block_43|\n",
      "|block_77|\n",
      "|block_19|\n",
      "|block_35|\n",
      "|block_53|\n",
      "|block_26|\n",
      "|block_20|\n",
      "|block_52|\n",
      "|block_12|\n",
      "|block_21|\n",
      "|block_36|\n",
      "|block_89|\n",
      "|block_84|\n",
      "|block_49|\n",
      "|block_93|\n",
      "|block_99|\n",
      "|block_18|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "block_64 0\n",
      "block_91 1\n",
      "block_39 2\n",
      "block_43 3\n",
      "block_77 4\n",
      "block_19 5\n",
      "block_35 6\n",
      "block_53 7\n",
      "block_26 8\n",
      "block_20 9\n",
      "block_52 10\n",
      "block_12 11\n",
      "block_21 12\n",
      "block_36 13\n",
      "block_89 14\n",
      "block_84 15\n",
      "block_49 16\n",
      "block_93 17\n",
      "block_99 18\n",
      "block_18 19\n",
      "block_44 20\n",
      "block_8 21\n",
      "block_71 22\n",
      "block_104 23\n",
      "block_4 24\n",
      "block_29 25\n",
      "block_38 26\n",
      "block_47 27\n",
      "block_48 28\n",
      "block_42 29\n",
      "block_85 30\n",
      "block_27 31\n",
      "block_108 32\n",
      "block_76 33\n",
      "block_1 34\n",
      "block_3 35\n",
      "block_56 36\n",
      "block_94 37\n",
      "block_72 38\n",
      "block_75 39\n",
      "block_78 40\n",
      "block_62 41\n",
      "block_101 42\n",
      "block_46 43\n",
      "block_110 44\n",
      "block_32 45\n",
      "block_51 46\n",
      "block_45 47\n",
      "block_59 48\n",
      "block_40 49\n",
      "block_15 50\n",
      "block_95 51\n",
      "block_96 52\n",
      "block_5 53\n",
      "block_68 54\n",
      "block_28 55\n",
      "block_97 56\n",
      "block_82 57\n",
      "block_69 58\n",
      "block_70 59\n",
      "block_61 60\n",
      "block_24 61\n",
      "block_33 62\n",
      "block_23 63\n",
      "block_31 64\n",
      "block_67 65\n",
      "block_0 66\n",
      "block_14 67\n",
      "block_41 68\n",
      "block_100 69\n",
      "block_22 70\n",
      "block_109 71\n",
      "block_83 72\n",
      "block_30 73\n",
      "block_88 74\n",
      "block_98 75\n",
      "block_106 76\n",
      "block_37 77\n",
      "block_10 78\n",
      "block_80 79\n",
      "block_103 80\n",
      "block_55 81\n",
      "block_73 82\n",
      "block_111 83\n",
      "block_86 84\n",
      "block_63 85\n",
      "block_105 86\n",
      "block_9 87\n",
      "block_92 88\n",
      "block_11 89\n",
      "block_54 90\n",
      "block_107 91\n",
      "block_6 92\n",
      "block_2 93\n",
      "block_66 94\n",
      "block_7 95\n",
      "block_25 96\n",
      "block_74 97\n",
      "block_87 98\n",
      "block_60 99\n",
      "block_57 100\n",
      "block_79 101\n",
      "block_16 102\n",
      "block_102 103\n",
      "block_13 104\n",
      "block_34 105\n",
      "block_58 106\n",
      "block_50 107\n",
      "block_17 108\n",
      "block_81 109\n",
      "block_90 110\n",
      "block_65 111\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "df_full = []\n",
    "df_file = household_mini.select(\"file\").distinct()\n",
    "df_file.show()\n",
    "count = 0\n",
    "for row in df_file.rdd.collect():\n",
    "    file = row.file\n",
    "    print(file,count)\n",
    "    count += 1\n",
    "    file_path = base_path + \"halfhourly_dataset/\"+ file+\".csv\"\n",
    "    half_hourly_consumption_data = sqlcontext.read.csv(file_path,header=True,inferSchema=True).cache()\n",
    "    half_hourly_consumption_data.dropna(how='any')\n",
    "    half_hourly_consumption_data = resample_to_1_hr(half_hourly_consumption_data)\n",
    "    if flag == 0:\n",
    "        df_full = sqlcontext.createDataFrame([],half_hourly_consumption_data.schema)\n",
    "        flag = 1\n",
    "    df_full = df_full.union(half_hourly_consumption_data)\n",
    "    df_full = df_full.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83919248"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLid_under_Consideration = sqlcontext.read.csv(base_path+\"Feature_file/Cleaned_2013_Features_mth_5.csv\",header=True)\n",
    "LCLid_under_Consideration = LCLid_under_Consideration.select(\"LCLid\").distinct()\n",
    "# LCLid_under_Consideration.count() #3930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full1 = df_full.filter((df_full.date >= date(2012,12,1)) & (df_full.date <= date(2013,12,31)))\n",
    "# df_full = df_full.filter((df_full.date >= date(2013,1,1)) & (df_full.date <= date(2013,12,31)))\n",
    "# df_full = df_full.na.drop()\n",
    "# print(df_full.count())\n",
    "# year_df = df_full.groupBy(\"LCLid\").count()\n",
    "# print(year_df.count())\n",
    "# year_df = year_df.filter(year_df[\"count\"] >= 8760 )\n",
    "# print(\"Total user in 2013 with full evidence \", year_df.select(\"LCLid\").distinct().count())\n",
    "# df_full = df_full1.join(broadcast(year_df),[\"LCLid\"])\n",
    "# df_full.count()\n",
    "\n",
    "df_full = df_full.join(broadcast(LCLid_under_Consideration),[\"LCLid\"])\n",
    "df_full = df_full.filter((df_full.date >= date(2012,12,1)) & (df_full.date <= date(2013,12,31)))\n",
    "# print(\"Total user in 2013 with full evidence \", df_full.select(\"LCLid\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------+--------------+-----+\n",
      "|    LCLid|               tstp|energy(kWh/hh)|count|\n",
      "+---------+-------------------+--------------+-----+\n",
      "|MAC000682|2012-12-01 00:00:00|        0.137 |17520|\n",
      "+---------+-------------------+--------------+-----+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_full.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- energy(kWh/h): double (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- weekDay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_file = sqlcontext.read.csv(base_path+\"cluster_info_k_means_k-8.csv\",inferSchema=True,header=True)\n",
    "LCLid_With_Cluster_id = df_full.join(broadcast(cluster_file),[\"LCLid\"])\n",
    "LCLid_With_Cluster_id = LCLid_With_Cluster_id.drop(\"_c0\",\"count\")\n",
    "# LCLid_With_Cluster_id.count()  #34422870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LCLid_With_Cluster_id.select(\"cluster_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling to 1 hour and extracting variable like month,hour,weekday etc.\n",
    "# df = LCLid_With_Cluster_id.select(\"cluster_id\",\"tstp\",\"energy(kWh/hh)\",date_format(\"tstp\",\"yyyy-MM-dd\").alias(\"date\"),date_format(\"tstp\",'HH:mm').alias(\"start time\"),date_format(\"tstp\",'E').alias(\"weekDay\"),month(\"tstp\").alias(\"month\"),hour(\"tstp\").alias(\"hour\"))\n",
    "# df = df.withColumn(\"energy(kWh/hh)\",df[\"energy(kWh/hh)\"].cast(DoubleType()))\n",
    "df1 = (LCLid_With_Cluster_id.groupby('cluster_id',\"date\",\"hour\").sum(\"energy(kWh/h)\")).orderBy('date','hour',ascending=True)\n",
    "df1 = df1.withColumnRenamed(\"sum(energy(kWh/h))\",\"energy(kWh/h)\")\n",
    "resampled_df = df1.select(\"cluster_id\",\"date\",\"hour\",\"energy(kWh/h)\",month(\"date\").alias(\"month\"),date_format(\"date\",'E').alias(\"weekDay\"))\n",
    "resampled_df = resampled_df.withColumn(\"energy(kWh/h)\", sf.round(resampled_df[\"energy(kWh/h)\"], 3))\n",
    "resampled_df = resampled_df.withColumn(\"date\", resampled_df[\"date\"].cast(DateType()))\n",
    "#resampled_df.count()    #70072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- visibility: double (nullable = true)\n",
      " |-- windBearing: integer (nullable = true)\n",
      " |-- dewPoint: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- apparentTemperature: double (nullable = true)\n",
      " |-- windSpeed: double (nullable = true)\n",
      " |-- precipType: string (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- hour1: integer (nullable = true)\n",
      " |-- date1: date (nullable = true)\n",
      " |-- date2: string (nullable = true)\n",
      " |-- temperatureMax: double (nullable = true)\n",
      " |-- temperatureMin: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- cluster_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- energy(kWh/h): double (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- weekDay: string (nullable = true)\n",
      " |-- 1_diff_energy_t_0: double (nullable = true)\n",
      " |-- 2_diff_energy_t_0: double (nullable = true)\n",
      " |-- 1_diff_energy_t_1: double (nullable = true)\n",
      " |-- 2_diff_energy_t_1: double (nullable = true)\n",
      " |-- 1_diff_energy_t_2: double (nullable = true)\n",
      " |-- 2_diff_energy_t_2: double (nullable = true)\n",
      " |-- 1_diff_energy_t_3: double (nullable = true)\n",
      " |-- 2_diff_energy_t_3: double (nullable = true)\n",
      " |-- 1_diff_energy_t_4: double (nullable = true)\n",
      " |-- 2_diff_energy_t_4: double (nullable = true)\n",
      " |-- 1_diff_energy_t_5: double (nullable = true)\n",
      " |-- 2_diff_energy_t_5: double (nullable = true)\n",
      " |-- 1_diff_energy_t_6: double (nullable = true)\n",
      " |-- 2_diff_energy_t_6: double (nullable = true)\n",
      " |-- diff_energy_week_t_1: double (nullable = true)\n",
      " |-- diff_energy_week_t_2: double (nullable = true)\n",
      " |-- diff_energy_week_t_3: double (nullable = true)\n",
      " |-- diff_energy_week_t_4: double (nullable = true)\n",
      " |-- rnk: integer (nullable = true)\n",
      " |-- mean_1: double (nullable = true)\n",
      " |-- mean_2: double (nullable = true)\n",
      " |-- mean_3: double (nullable = true)\n",
      " |-- mean_4: double (nullable = true)\n",
      " |-- mean_5: double (nullable = true)\n",
      " |-- mean_6: double (nullable = true)\n",
      " |-- min_1: double (nullable = true)\n",
      " |-- max_1: double (nullable = true)\n",
      " |-- min_2: double (nullable = true)\n",
      " |-- max_2: double (nullable = true)\n",
      " |-- min_3: double (nullable = true)\n",
      " |-- max_3: double (nullable = true)\n",
      " |-- min_4: double (nullable = true)\n",
      " |-- max_4: double (nullable = true)\n",
      " |-- min_5: double (nullable = true)\n",
      " |-- max_5: double (nullable = true)\n",
      " |-- min_6: double (nullable = true)\n",
      " |-- max_6: double (nullable = true)\n",
      " |-- visibility: double (nullable = true)\n",
      " |-- windBearing: integer (nullable = true)\n",
      " |-- dewPoint: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- apparentTemperature: double (nullable = true)\n",
      " |-- windSpeed: double (nullable = true)\n",
      " |-- precipType: string (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- date2: string (nullable = true)\n",
      " |-- temperatureMax: double (nullable = true)\n",
      " |-- temperatureMin: double (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- Weekday/end: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_df = prepare_dataset(resampled_df,6,4) #70072\n",
    "# print(feature_df.count())\n",
    "add_weather_feature_df = add_weather_feature(feature_df)\n",
    "add_holiday_feature_df = add_holiday_feature(add_weather_feature_df)\n",
    "# add_acorn_info_df = acorn_info(add_holiday_feature_df,household_info)\n",
    "add_acorn_info_df = add_holiday_feature_df\n",
    "add_acorn_info_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70592"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_acorn_info_df.count()       #64648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70592\n"
     ]
    }
   ],
   "source": [
    "final_feature_df = add_acorn_info_df\n",
    "print(final_feature_df.count())\n",
    "inputCols = [\"weekDay\",\"precipType\",\"summary\",]\n",
    "outputCols = [\"weekDay_index\",\"precipType_index\",\"summary_index\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(final_feature_df) for column in inputCols ]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df = pipeline.fit(final_feature_df).transform(final_feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70016"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.filter(df.date>=date(2013,1,1))\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().to_csv(base_path+\"k-means-8-feature.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
