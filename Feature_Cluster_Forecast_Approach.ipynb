{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import functions as sf\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql.functions import col, avg, date_format,month,hour,lag, date_sub,lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType, DoubleType\n",
    "from pyspark.sql.functions import broadcast\n",
    "import pandas as pd\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, OneHotEncoder\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://master:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://172.16.27.208:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=spark://172.16.27.208:7077 appName=spark>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.stop()\n",
    "sc = pyspark.SparkContext(master=\"spark://172.16.27.208:7077\",appName=\"spark\")\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(resampled_df,window_period,week_lag):\n",
    "    # window period = 6 for lag input for same hour,same-1,same-2 \n",
    "    window = Window.partitionBy('cluster_id').orderBy('date','hour')\n",
    "    for lag_hour in range(0,3):\n",
    "        for diff in range(1,window_period+1):\n",
    "            resampled_df = resampled_df.withColumn('{}_diff_energy_t_{}'.format(diff,lag_hour),lag(resampled_df['energy(kWh/h)'], count=24*diff+lag_hour).over(window)) \n",
    "    for lag_week in range(1,week_lag+1):\n",
    "        resampled_df = resampled_df.withColumn('diff_energy_week_t_{}'.format(lag_week),lag(resampled_df['energy(kWh/h)'], count=24*7*lag_week).over(window)) \n",
    "    df_resample_lag = resampled_df\n",
    "\n",
    "    # Mean of previous 6 days\n",
    "    df_resample_lag = df_resample_lag.withColumn(\"rnk\",sf.dense_rank().over(Window.partitionBy('cluster_id').orderBy('date')))\n",
    "    for days in range(1,window_period+1):\n",
    "        df_resample_lag = df_resample_lag.withColumn(\"mean_{}\".format(days),avg(\"energy(kWh/h)\").over(Window.partitionBy(\"cluster_id\").orderBy(\"rnk\").rangeBetween(-days,-days)))\n",
    "#     df_resample_lag.show()\n",
    "    # Min power of previous 2 days\n",
    "    for days in range(1,window_period+1):\n",
    "        df_resample_lag = df_resample_lag.withColumn(\"min_{}\".format(days),sf.min(\"energy(kWh/h)\").over(Window.partitionBy(\"cluster_id\").orderBy(\"rnk\").rangeBetween(-days,-days)))\n",
    "        df_resample_lag = df_resample_lag.withColumn(\"max_{}\".format(days),sf.max(\"energy(kWh/h)\").over(Window.partitionBy(\"cluster_id\").orderBy(\"rnk\").rangeBetween(-days,-days)))\n",
    "    \n",
    "    return df_resample_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weather_feature(df):\n",
    "    weather_data = sqlcontext.read.csv(base_path+\"weather_hourly_darksky.csv\",header=True,inferSchema=True)\n",
    "    weather_daily_data = sqlcontext.read.csv(base_path+\"weather_daily_darksky.csv\",header=True,inferSchema=True)\n",
    "    weather_daily_data = weather_daily_data.select(date_format(\"temperatureMaxTime\",\"yyyy-MM-dd\").alias(\"date2\"),\"temperatureMax\",\"temperatureMin\")\n",
    "    weather_data = weather_data.withColumn(\"hour1\",hour(weather_data[\"time\"]))\n",
    "    weather_data = weather_data.withColumn(\"date1\",date_format(weather_data[\"time\"],\"yyyy-MM-dd\").cast(DateType()))\n",
    "    weather_data = weather_data.drop(\"time\",\"icon\",\"temperature\")\n",
    "    weather_data = weather_data.join(broadcast(weather_daily_data),(weather_daily_data.date2 == weather_data.date1))\n",
    "    weather_data.printSchema()\n",
    "    df_full_dataset = df.join(broadcast(weather_data),(df.date == weather_data.date1) & (df.hour == weather_data.hour1))\n",
    "    df_full_dataset = df_full_dataset.drop(\"hour1\",\"date1\").cache()\n",
    "    df_full_dataset.take(1)\n",
    "    df_full_dataset = df_full_dataset.na.drop()\n",
    "    return df_full_dataset\n",
    "\n",
    "def add_holiday_feature(df):\n",
    "    holiday_data = sqlcontext.read.csv(base_path+\"uk_bank_holidays.csv\",header=True,inferSchema=True)\n",
    "    holiday_data = holiday_data.withColumn(\"Bank holidays\",date_format(holiday_data[\"Bank holidays\"],\"yyyy-MM-dd\").cast(DateType()))\n",
    "    holiday_data = holiday_data.select(\"Bank holidays\")\n",
    "    holiday_data = holiday_data.withColumn(\"holiday\",lit(1))\n",
    "    feature_df = df.join(holiday_data,holiday_data[\"Bank holidays\"] == df[\"date\"],how=\"left\")\n",
    "    feature_df = feature_df.fillna({'holiday':'0'})\n",
    "    feature_df = feature_df.drop(\"Bank holidays\")\n",
    "    feature_df = feature_df.withColumn(\"Weekday/end\",sf.when((col(\"weekDay\")==str(\"Sat\")) | (col(\"weekDay\")==str(\"Sat\")),1).otherwise(0))\n",
    "    feature_df = feature_df.na.drop()\n",
    "    return feature_df\n",
    "\n",
    "# def acorn_info(df,household_info):\n",
    "#     Acorn_data_group = household_info.select(\"LCLid\",\"stdorToU\",\"Acorn_grouped\")\n",
    "#     Acorn_data_group.select(\"Acorn_grouped\").distinct().collect()\n",
    "#     possible_group = [\"Comfortable\",\"Affluent\",\"Adversity\"]\n",
    "#     Acorn_data_group = Acorn_data_group.filter(Acorn_data_group.Acorn_grouped.isin(possible_group))\n",
    "#     feature_df = df.join(Acorn_data_group,[\"LCLid\"])                # preventing duplicate column in df\n",
    "#     feature_df.printSchema()\n",
    "#     #Acorn_data_group.select(\"stdorToU\").distinct().collect()\n",
    "#     feature_df.take(1)\n",
    "#     return feature_df\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/test5/Desktop/smart-meters-in-london/\"\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_info = sqlcontext.read.csv(base_path+\"informations_households.csv\",header=True,inferSchema=True)\n",
    "household_mini = household_info\n",
    "# household_mini = household_info.limit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- stdorToU: string (nullable = true)\n",
      " |-- Acorn: string (nullable = true)\n",
      " |-- Acorn_grouped: string (nullable = true)\n",
      " |-- file: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "household_mini.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_to_1_hr(df):\n",
    "    df = df.select(\"LCLid\",\"tstp\",\"energy(kWh/hh)\",date_format(\"tstp\",\"yyyy-MM-dd\").alias(\"date\"),date_format(\"tstp\",'HH:mm').alias(\"start time\"),date_format(\"tstp\",'E').alias(\"weekDay\"),month(\"tstp\").alias(\"month\"),hour(\"tstp\").alias(\"hour\"))\n",
    "    df = df.withColumn(\"energy(kWh/hh)\",df[\"energy(kWh/hh)\"].cast(\"float\"))\n",
    "    df1 = (df.groupby('LCLid',\"date\",\"hour\").sum(\"energy(kWh/hh)\")).orderBy('date','hour',ascending=True)\n",
    "    df1 = df1.withColumnRenamed(\"sum(energy(kWh/hh))\",\"energy(kWh/h)\")\n",
    "    resampled_df = df1.select(\"LCLid\",\"date\",\"hour\",\"energy(kWh/h)\",month(\"date\").alias(\"month\"),date_format(\"date\",'E').alias(\"weekDay\"))\n",
    "    resampled_df = resampled_df.withColumn(\"energy(kWh/h)\", sf.round(resampled_df[\"energy(kWh/h)\"], 3))\n",
    "    resampled_df = resampled_df.withColumn(\"date\", resampled_df[\"date\"].cast(DateType()))\n",
    "    return resampled_df\n",
    "# resampled_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_64 0\n",
      "block_91 1\n",
      "block_39 2\n",
      "block_43 3\n",
      "block_77 4\n",
      "block_19 5\n",
      "block_35 6\n",
      "block_53 7\n",
      "block_26 8\n",
      "block_20 9\n",
      "block_52 10\n",
      "block_12 11\n",
      "block_21 12\n",
      "block_36 13\n",
      "block_89 14\n",
      "block_84 15\n",
      "block_49 16\n",
      "block_93 17\n",
      "block_99 18\n",
      "block_18 19\n",
      "block_44 20\n",
      "block_8 21\n",
      "block_71 22\n",
      "block_104 23\n",
      "block_4 24\n",
      "block_29 25\n",
      "block_38 26\n",
      "block_47 27\n",
      "block_48 28\n",
      "block_42 29\n",
      "block_85 30\n",
      "block_27 31\n",
      "block_108 32\n",
      "block_76 33\n",
      "block_1 34\n",
      "block_3 35\n",
      "block_56 36\n",
      "block_94 37\n",
      "block_72 38\n",
      "block_75 39\n",
      "block_78 40\n",
      "block_62 41\n",
      "block_101 42\n",
      "block_46 43\n",
      "block_110 44\n",
      "block_32 45\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "df_full = []\n",
    "df_file = household_mini.select(\"file\").distinct()\n",
    "# df_file.show()\n",
    "count = 0\n",
    "for row in df_file.rdd.collect():\n",
    "    file = row.file\n",
    "    print(file,count)\n",
    "    count += 1\n",
    "    file_path = base_path + \"halfhourly_dataset/\"+ file+\".csv\"\n",
    "    half_hourly_consumption_data = sqlcontext.read.csv(file_path,header=True,inferSchema=True).cache()\n",
    "    half_hourly_consumption_data.dropna(how='any')\n",
    "    half_hourly_consumption_data = resample_to_1_hr(half_hourly_consumption_data)\n",
    "    if flag == 0:\n",
    "        df_full = sqlcontext.createDataFrame([],half_hourly_consumption_data.schema)\n",
    "        flag = 1\n",
    "    df_full = df_full.union(half_hourly_consumption_data)\n",
    "    df_full = df_full.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83919248"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LCLid_under_Consideration = sqlcontext.read.csv(base_path+\"Feature_file/Cleaned_2013_Features_mth_5.csv\",header=True)\n",
    "LCLid_under_Consideration = LCLid_under_Consideration.select(\"LCLid\").distinct()\n",
    "# LCLid_under_Consideration.count() #3930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_full1 = df_full.filter((df_full.date >= date(2012,12,1)) & (df_full.date <= date(2013,12,31)))\n",
    "# df_full = df_full.filter((df_full.date >= date(2013,1,1)) & (df_full.date <= date(2013,12,31)))\n",
    "# df_full = df_full.na.drop()\n",
    "# print(df_full.count())\n",
    "# year_df = df_full.groupBy(\"LCLid\").count()\n",
    "# print(year_df.count())\n",
    "# year_df = year_df.filter(year_df[\"count\"] >= 8760 )\n",
    "# print(\"Total user in 2013 with full evidence \", year_df.select(\"LCLid\").distinct().count())\n",
    "# df_full = df_full1.join(broadcast(year_df),[\"LCLid\"])\n",
    "# df_full.count()\n",
    "\n",
    "df_full = df_full.join(broadcast(LCLid_under_Consideration),[\"LCLid\"])\n",
    "df_full = df_full.filter((df_full.date >= date(2012,12,1)) & (df_full.date <= date(2013,12,31)))\n",
    "# print(\"Total user in 2013 with full evidence \", df_full.select(\"LCLid\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+----+-------------+-----+-------+\n",
      "|    LCLid|      date|hour|energy(kWh/h)|month|weekDay|\n",
      "+---------+----------+----+-------------+-----+-------+\n",
      "|MAC003198|2012-12-01|   0|        0.108|   12|    Sat|\n",
      "+---------+----------+----+-------------+-----+-------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_full.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- energy(kWh/h): double (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- weekDay: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_full.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_file = sqlcontext.read.csv(base_path+\"cluster_info_b-k_means_k-8.csv\",inferSchema=True,header=True)\n",
    "LCLid_With_Cluster_id = df_full.join(broadcast(cluster_file),[\"LCLid\"])\n",
    "LCLid_With_Cluster_id = LCLid_With_Cluster_id.drop(\"_c0\",\"count\")\n",
    "# LCLid_With_Cluster_id.count()  #34422870"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LCLid_With_Cluster_id.select(\"cluster_id\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling to 1 hour and extracting variable like month,hour,weekday etc.\n",
    "# df = LCLid_With_Cluster_id.select(\"cluster_id\",\"tstp\",\"energy(kWh/hh)\",date_format(\"tstp\",\"yyyy-MM-dd\").alias(\"date\"),date_format(\"tstp\",'HH:mm').alias(\"start time\"),date_format(\"tstp\",'E').alias(\"weekDay\"),month(\"tstp\").alias(\"month\"),hour(\"tstp\").alias(\"hour\"))\n",
    "# df = df.withColumn(\"energy(kWh/hh)\",df[\"energy(kWh/hh)\"].cast(DoubleType()))\n",
    "df1 = (LCLid_With_Cluster_id.groupby('cluster_id',\"date\",\"hour\").sum(\"energy(kWh/h)\")).orderBy('date','hour',ascending=True)\n",
    "df1 = df1.withColumnRenamed(\"sum(energy(kWh/h))\",\"energy(kWh/h)\")\n",
    "resampled_df = df1.select(\"cluster_id\",\"date\",\"hour\",\"energy(kWh/h)\",month(\"date\").alias(\"month\"),date_format(\"date\",'E').alias(\"weekDay\"))\n",
    "resampled_df = resampled_df.withColumn(\"energy(kWh/h)\", sf.round(resampled_df[\"energy(kWh/h)\"], 3))\n",
    "resampled_df = resampled_df.withColumn(\"date\", resampled_df[\"date\"].cast(DateType()))\n",
    "#resampled_df.count()    #70072"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- visibility: double (nullable = true)\n",
      " |-- windBearing: integer (nullable = true)\n",
      " |-- dewPoint: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- apparentTemperature: double (nullable = true)\n",
      " |-- windSpeed: double (nullable = true)\n",
      " |-- precipType: string (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- hour1: integer (nullable = true)\n",
      " |-- date1: date (nullable = true)\n",
      " |-- date2: string (nullable = true)\n",
      " |-- temperatureMax: double (nullable = true)\n",
      " |-- temperatureMin: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- cluster_id: integer (nullable = true)\n",
      " |-- date: date (nullable = true)\n",
      " |-- hour: integer (nullable = true)\n",
      " |-- energy(kWh/h): double (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- weekDay: string (nullable = true)\n",
      " |-- 1_diff_energy_t_0: double (nullable = true)\n",
      " |-- 2_diff_energy_t_0: double (nullable = true)\n",
      " |-- 3_diff_energy_t_0: double (nullable = true)\n",
      " |-- 4_diff_energy_t_0: double (nullable = true)\n",
      " |-- 5_diff_energy_t_0: double (nullable = true)\n",
      " |-- 6_diff_energy_t_0: double (nullable = true)\n",
      " |-- 1_diff_energy_t_1: double (nullable = true)\n",
      " |-- 2_diff_energy_t_1: double (nullable = true)\n",
      " |-- 3_diff_energy_t_1: double (nullable = true)\n",
      " |-- 4_diff_energy_t_1: double (nullable = true)\n",
      " |-- 5_diff_energy_t_1: double (nullable = true)\n",
      " |-- 6_diff_energy_t_1: double (nullable = true)\n",
      " |-- 1_diff_energy_t_2: double (nullable = true)\n",
      " |-- 2_diff_energy_t_2: double (nullable = true)\n",
      " |-- 3_diff_energy_t_2: double (nullable = true)\n",
      " |-- 4_diff_energy_t_2: double (nullable = true)\n",
      " |-- 5_diff_energy_t_2: double (nullable = true)\n",
      " |-- 6_diff_energy_t_2: double (nullable = true)\n",
      " |-- diff_energy_week_t_1: double (nullable = true)\n",
      " |-- diff_energy_week_t_2: double (nullable = true)\n",
      " |-- diff_energy_week_t_3: double (nullable = true)\n",
      " |-- diff_energy_week_t_4: double (nullable = true)\n",
      " |-- rnk: integer (nullable = true)\n",
      " |-- mean_1: double (nullable = true)\n",
      " |-- mean_2: double (nullable = true)\n",
      " |-- mean_3: double (nullable = true)\n",
      " |-- mean_4: double (nullable = true)\n",
      " |-- mean_5: double (nullable = true)\n",
      " |-- mean_6: double (nullable = true)\n",
      " |-- min_1: double (nullable = true)\n",
      " |-- max_1: double (nullable = true)\n",
      " |-- min_2: double (nullable = true)\n",
      " |-- max_2: double (nullable = true)\n",
      " |-- min_3: double (nullable = true)\n",
      " |-- max_3: double (nullable = true)\n",
      " |-- min_4: double (nullable = true)\n",
      " |-- max_4: double (nullable = true)\n",
      " |-- min_5: double (nullable = true)\n",
      " |-- max_5: double (nullable = true)\n",
      " |-- min_6: double (nullable = true)\n",
      " |-- max_6: double (nullable = true)\n",
      " |-- visibility: double (nullable = true)\n",
      " |-- windBearing: integer (nullable = true)\n",
      " |-- dewPoint: double (nullable = true)\n",
      " |-- pressure: double (nullable = true)\n",
      " |-- apparentTemperature: double (nullable = true)\n",
      " |-- windSpeed: double (nullable = true)\n",
      " |-- precipType: string (nullable = true)\n",
      " |-- humidity: double (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- date2: string (nullable = true)\n",
      " |-- temperatureMax: double (nullable = true)\n",
      " |-- temperatureMin: double (nullable = true)\n",
      " |-- holiday: integer (nullable = true)\n",
      " |-- Weekday/end: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "feature_df = prepare_dataset(resampled_df,7,4) #70072\n",
    "# print(feature_df.count())\n",
    "add_weather_feature_df = add_weather_feature(feature_df)\n",
    "add_holiday_feature_df = add_holiday_feature(add_weather_feature_df)\n",
    "# add_acorn_info_df = acorn_info(add_holiday_feature_df,household_info)\n",
    "add_acorn_info_df = add_holiday_feature_df\n",
    "add_acorn_info_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70592"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "add_acorn_info_df.count()       #64648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70592\n"
     ]
    }
   ],
   "source": [
    "final_feature_df = add_acorn_info_df\n",
    "print(final_feature_df.count())\n",
    "inputCols = [\"weekDay\",\"precipType\",\"summary\",]\n",
    "outputCols = [\"weekDay_index\",\"precipType_index\",\"summary_index\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(final_feature_df) for column in inputCols ]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df = pipeline.fit(final_feature_df).transform(final_feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70016"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.filter(df.date>=date(2013,1,1))\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.toPandas().to_csv(base_path+\"bk-means-8-feature_6days.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(cluster_id=1, date=datetime.date(2013, 1, 1), hour=0, energy(kWh/h)=181.633, month=1, weekDay='Tue', 1_diff_energy_t_0=163.88, 2_diff_energy_t_0=171.13, 3_diff_energy_t_0=169.788, 4_diff_energy_t_0=165.544, 5_diff_energy_t_0=158.319, 6_diff_energy_t_0=158.918, 1_diff_energy_t_1=187.865, 2_diff_energy_t_1=185.326, 3_diff_energy_t_1=187.275, 4_diff_energy_t_1=183.415, 5_diff_energy_t_1=166.311, 6_diff_energy_t_1=172.453, 1_diff_energy_t_2=224.507, 2_diff_energy_t_2=227.34, 3_diff_energy_t_2=221.904, 4_diff_energy_t_2=217.02, 5_diff_energy_t_2=207.147, 6_diff_energy_t_2=193.731, diff_energy_week_t_1=154.258, diff_energy_week_t_2=154.541, diff_energy_week_t_3=161.117, diff_energy_week_t_4=160.356, rnk=32, mean_1=181.54587500000002, mean_2=184.17049999999998, mean_3=183.41945833333332, mean_4=179.73729166666666, mean_5=176.49670833333334, mean_6=170.50029166666664, min_1=97.008, max_1=282.125, min_2=93.813, max_2=271.852, min_3=95.023, max_3=279.498, min_4=96.869, max_4=274.422, min_5=99.437, max_5=273.424, min_6=96.444, max_6=246.789, visibility=13.28, windBearing=269, dewPoint=2.6, pressure=1008.19, apparentTemperature=3.66, windSpeed=5.46, precipType='rain', humidity=0.73, summary='Partly Cloudy', date2='2013-01-01', temperatureMax=7.49, temperatureMin=3.31, holiday=1, Weekday/end=0, weekDay_index=5.0, precipType_index=0.0, summary_index=2.0)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Invalid alias: The name clear can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name more can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name less can't be aliased because it is another magic command.\n",
      "ERROR:root:Invalid alias: The name man can't be aliased because it is another magic command.\n"
     ]
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
