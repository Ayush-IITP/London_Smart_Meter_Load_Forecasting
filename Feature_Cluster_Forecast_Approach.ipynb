{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from matplotlib import pyplot as plt\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import DateType\n",
    "import pandas as pd\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer, OneHotEncoder\n",
    "from datetime import datetime, timedelta, date\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc.stop()\n",
    "# sc = pyspark.SparkContext(master=\"spark://192.168.1.3:7077\",appName=\"spark\")\n",
    "# sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(resampled_df,window_period,week_lag):\n",
    "    # window period = 6 for lag input for same hour,same-1,same-2 \n",
    "    window = Window.partitionBy('cluster_id').orderBy('date','hour')\n",
    "    for lag_hour in range(0,window_period+1):\n",
    "        for diff in range(1,3):\n",
    "            resampled_df = resampled_df.withColumn('{}_diff_energy_t_{}'.format(diff,lag_hour),lag(resampled_df['energy(kWh/h)'], count=24*diff+lag_hour).over(window)) \n",
    "    for lag_week in range(1,week_lag+1):\n",
    "        resampled_df = resampled_df.withColumn('diff_energy_week_t_{}'.format(lag_week),lag(resampled_df['energy(kWh/h)'], count=24*7*lag_week).over(window)) \n",
    "    df_resample_lag = resampled_df\n",
    "\n",
    "    # Mean of previous 6 days\n",
    "    df_resample_lag = df_resample_lag.withColumn(\"rnk\",sf.dense_rank().over(Window.partitionBy('cluster_id').orderBy('date')))\n",
    "    for days in range(1,window_period+1):\n",
    "        df_resample_lag = df_resample_lag.withColumn(\"mean_{}\".format(days),avg(\"energy(kWh/h)\").over(Window.partitionBy(\"cluster_id\").orderBy(\"rnk\").rangeBetween(-days,-days)))\n",
    "#     df_resample_lag.show()\n",
    "    # Min power of previous 2 days\n",
    "    for days in range(1,window_period+1):\n",
    "        df_resample_lag = df_resample_lag.withColumn(\"min_{}\".format(days),sf.min(\"energy(kWh/h)\").over(Window.partitionBy(\"cluster_id\").orderBy(\"rnk\").rangeBetween(-days,-days)))\n",
    "        df_resample_lag = df_resample_lag.withColumn(\"max_{}\".format(days),sf.max(\"energy(kWh/h)\").over(Window.partitionBy(\"cluster_id\").orderBy(\"rnk\").rangeBetween(-days,-days)))\n",
    "    \n",
    "    return df_resample_lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weather_feature(df):\n",
    "    weather_data = sqlcontext.read.csv(base_path+\"weather_hourly_darksky.csv\",header=True,inferSchema=True)\n",
    "    weather_daily_data = sqlcontext.read.csv(base_path+\"weather_daily_darksky.csv\",header=True,inferSchema=True)\n",
    "    weather_daily_data = weather_daily_data.select(date_format(\"temperatureMaxTime\",\"yyyy-MM-dd\").alias(\"date2\"),\"temperatureMax\",\"temperatureMin\")\n",
    "    weather_data = weather_data.withColumn(\"hour1\",hour(weather_data[\"time\"]))\n",
    "    weather_data = weather_data.withColumn(\"date1\",date_format(weather_data[\"time\"],\"yyyy-MM-dd\").cast(DateType()))\n",
    "    weather_data = weather_data.drop(\"time\",\"icon\",\"temperature\")\n",
    "    weather_data = weather_data.join(broadcast(weather_daily_data),(weather_daily_data.date2 == weather_data.date1))\n",
    "    weather_data.printSchema()\n",
    "    df_full_dataset = df.join(broadcast(weather_data),(df.date == weather_data.date1) & (df.hour == weather_data.hour1))\n",
    "    df_full_dataset = df_full_dataset.drop(\"hour1\",\"date1\").cache()\n",
    "    df_full_dataset.take(1)\n",
    "    return df_full_dataset\n",
    "\n",
    "def add_holiday_feature(df):\n",
    "    holiday_data = sqlcontext.read.csv(base_path+\"uk_bank_holidays.csv\",header=True,inferSchema=True)\n",
    "    holiday_data = holiday_data.withColumn(\"Bank holidays\",date_format(holiday_data[\"Bank holidays\"],\"yyyy-MM-dd\").cast(DateType()))\n",
    "    holiday_data = holiday_data.select(\"Bank holidays\")\n",
    "    holiday_data = holiday_data.withColumn(\"holiday\",lit(1))\n",
    "    feature_df = df.join(holiday_data,holiday_data[\"Bank holidays\"] == df_full_dataset[\"date\"],how=\"left\")\n",
    "    feature_df = feature_df.fillna({'holiday':'0'})\n",
    "    feature_df = feature_df.drop(\"Bank holidays\")\n",
    "    feature_df = feature_df.withColumn(\"Weekday/end\",sf.when((col(\"weekDay\")==str(\"Sat\")) | (col(\"weekDay\")==str(\"Sat\")),1).otherwise(0))\n",
    "    return feature_df\n",
    "\n",
    "def acorn_info(df,household_info):\n",
    "    Acorn_data_group = household_info.select(\"LCLid\",\"stdorToU\",\"Acorn_grouped\")\n",
    "    Acorn_data_group.select(\"Acorn_grouped\").distinct().collect()\n",
    "    possible_group = [\"Comfortable\",\"Affluent\",\"Adversity\"]\n",
    "    Acorn_data_group = Acorn_data_group.filter(Acorn_data_group.Acorn_grouped.isin(possible_group))\n",
    "    feature_df = df.join(Acorn_data_group,[\"LCLid\"])                # preventing duplicate column in df\n",
    "    feature_df.printSchema()\n",
    "    #Acorn_data_group.select(\"stdorToU\").distinct().collect()\n",
    "    feature_df.take(1)\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/home/darkmatter/Desktop/smart-meters-in-london/\"\n",
    "sqlcontext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "household_info = sqlcontext.read.csv(base_path+\"informations_households.csv\",header=True,inferSchema=True)\n",
    "household_mini = household_info\n",
    "household_mini = household_info.limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- LCLid: string (nullable = true)\n",
      " |-- stdorToU: string (nullable = true)\n",
      " |-- Acorn: string (nullable = true)\n",
      " |-- Acorn_grouped: string (nullable = true)\n",
      " |-- file: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "household_mini.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|   file|\n",
      "+-------+\n",
      "|block_0|\n",
      "+-------+\n",
      "\n",
      "block_0 0\n"
     ]
    }
   ],
   "source": [
    "flag = 0\n",
    "df_full = []\n",
    "df_file = household_mini.select(\"file\").distinct()\n",
    "df_file.show()\n",
    "count = 0\n",
    "for row in df_file.rdd.collect():\n",
    "    file = row.file\n",
    "    print(file,count)\n",
    "    count += 1\n",
    "    file_path = base_path + \"halfhourly_dataset/\"+ file+\".csv\"\n",
    "    half_hourly_consumption_data = sqlcontext.read.csv(file_path,header=True,inferSchema=True).cache()\n",
    "    half_hourly_consumption_data.dropna(how='any')\n",
    "    if flag == 0:\n",
    "        df_full = sqlcontext.createDataFrame([],half_hourly_consumption_data.schema)\n",
    "        flag = 1\n",
    "    df_full = df_full.union(half_hourly_consumption_data)\n",
    "    df_full = df_full.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = df_full.filter((date_format(dataset_df.tstp,\"yyyy-MM-dd\") >= date(2013,1,1)) & (date_format(dataset_df.tstp,\"yyyy-MM-dd\") <= date(2013,12,31)))\n",
    "year_df = (dataset_df.groupBy(\"LCLid\").count()).filter(year_df[\"count\"] >= 8760 )\n",
    "print(\"Total user in 2013 with full evidence \", year_df.select(\"LCLid\").distinct().count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: file:/home/darkmatter/Desktop/smart-meters-in-london/cluster_info.csv;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib/Spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o82.csv.\n: org.apache.spark.sql.AnalysisException: Path does not exist: file:/home/darkmatter/Desktop/smart-meters-in-london/cluster_info.csv;\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:715)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$15.apply(DataSource.scala:389)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:388)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:239)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:227)\n\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:594)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:748)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-ec91dda8f1f5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcluster_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"cluster_info.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mLCLid_With_Cluster_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcluster_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"LCLid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/Spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mcsv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping)\u001b[0m\n\u001b[1;32m    437\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.5/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/Spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: file:/home/darkmatter/Desktop/smart-meters-in-london/cluster_info.csv;'"
     ]
    }
   ],
   "source": [
    "cluster_file = sqlContext.read.csv(base_path+\"cluster_info.csv\")\n",
    "LCLid_With_Cluster_id = df_full.join(broadcast(cluster_file),[\"LCLid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LCLid_With_Cluster_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f4cafad69d53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Resampling to 1 hour and extracting variable like month,hour,weekday etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLCLid_With_Cluster_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cluster_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"tstp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"energy(kWh/hh)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tstp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"yyyy-MM-dd\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tstp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'HH:mm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start time\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tstp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'E'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weekDay\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tstp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"month\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhour\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tstp\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"hour\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cluster_id'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"hour\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"energy(kWh/hh)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morderBy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'hour'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumnRenamed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sum(energy(kWh/hh))\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"energy(kWh/h)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mresampled_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cluster_id\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"hour\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"energy(kWh/h)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmonth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"month\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdate_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"date\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'E'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"weekDay\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'LCLid_With_Cluster_id' is not defined"
     ]
    }
   ],
   "source": [
    "# Resampling to 1 hour and extracting variable like month,hour,weekday etc.\n",
    "df = LCLid_With_Cluster_id.select(\"cluster_id\",\"tstp\",\"energy(kWh/hh)\",date_format(\"tstp\",\"yyyy-MM-dd\").alias(\"date\"),date_format(\"tstp\",'HH:mm').alias(\"start time\"),date_format(\"tstp\",'E').alias(\"weekDay\"),month(\"tstp\").alias(\"month\"),hour(\"tstp\").alias(\"hour\"))\n",
    "df1 = (df.groupby('cluster_id',\"date\",\"hour\").sum(\"energy(kWh/hh)\")).orderBy('date','hour',ascending=True)\n",
    "df1 = df1.withColumnRenamed(\"sum(energy(kWh/hh))\",\"energy(kWh/h)\")\n",
    "resampled_df = df1.select(\"cluster_id\",\"date\",\"hour\",\"energy(kWh/h)\",month(\"date\").alias(\"month\"),date_format(\"date\",'E').alias(\"weekDay\"))\n",
    "resampled_df = resampled_df.withColumn(\"energy(kWh/h)\", sf.round(resampled_df[\"energy(kWh/h)\"], 3))\n",
    "resampled_df = resampled_df.withColumn(\"date\", resampled_df[\"date\"].cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'prepare_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cb622b120367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeature_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresampled_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mAdd_on_Feature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_weather_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'prepare_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "feature_df = prepare_dataset(resampled_df,6,4)\n",
    "add_weather_feature_df = add_weather_feature(feature_df)\n",
    "add_holiday_feature_df = add_holiday_feature(add_weather_feature_df)\n",
    "add_acorn_info_df = acorn_info(add_holiday_feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_acorn_info_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-6f07e29d85f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_feature_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_acorn_info_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0minputCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"weekDay\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"precipType\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"stdorToU\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Acorn_grouped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moutputCols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"weekDay_index\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"precipType_index\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"summary_index\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"stdorToU_index\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Acorn_grouped_index\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mindexers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mStringIndexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\"_index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_df\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputCols\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'add_acorn_info_df' is not defined"
     ]
    }
   ],
   "source": [
    "final_feature_df = add_acorn_info_df\n",
    "inputCols = [\"weekDay\",\"precipType\",\"summary\",\"stdorToU\",\"Acorn_grouped\"]\n",
    "outputCols = [\"weekDay_index\",\"precipType_index\",\"summary_index\",\"stdorToU_index\",\"Acorn_grouped_index\"]\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(feature_df) for column in inputCols ]\n",
    "pipeline = Pipeline(stages=indexers)\n",
    "df = pipeline.fit(final_feature_df).transform(final_feature_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
